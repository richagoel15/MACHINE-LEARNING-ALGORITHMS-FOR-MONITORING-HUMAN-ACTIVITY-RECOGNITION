{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### PERFORMANCE ANALYSIS OF MACHINE LEARNING ALGORITHMS FOR MONITORING HUMAN ACTIVITY RECOGNITION \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTIVITY RECOGNITION FROM WIRELESS WEARABLESENSORS FOR ELDERLY PEOPLE USING ML TECHNIQUES & COMPARISON BETWEEN ALGORITHM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this thesis is to predict human behavior based upon wearable sensor data and Smartphone data and perform a comparative study of performance between multiple classification algorithms. We will be using publicly available UCI data set for this study. As this data is not derived from our experiment, so it becomes crucial to check the dataset for all the data quality issues. Otherwise, data quality issues can provide irrelevant results, so in order to prepare data for building models,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data load\n",
    "2. Standardization of the datset\n",
    "3. Splitting of Original Dataset between test and train\n",
    "4. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** 1. Data Load***\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import all the required Library\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import missingno as msno \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold  \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 300)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'KNN_Performance.csv' does not exist: b'KNN_Performance.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e1ef7316cbcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Load the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_DNN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DNN Classifier.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_KNN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"KNN_Performance.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_logreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistics_Performance.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_rf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Random_Performance.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'KNN_Performance.csv' does not exist: b'KNN_Performance.csv'"
     ]
    }
   ],
   "source": [
    "## Load the data \n",
    "df_DNN=pd.read_csv(\"DNN Classifier.csv\")\n",
    "df_KNN=pd.read_csv(\"KNN_Performance.csv\")\n",
    "df_logreg=pd.read_csv(\"Logistics_Performance.csv\")\n",
    "df_rf=pd.read_csv(\"Random_Performance.csv\")\n",
    "df_svm=pd.read_csv(\"SVM_Performance.csv\")\n",
    "df_gbm=pd.read_csv(\"GBM Classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=df_DNN.append(df_KNN)\n",
    "df_data=df_data.append(df_logreg)\n",
    "df_data=df_data.append(df_rf)\n",
    "df_data=df_data.append(df_svm)\n",
    "df_data=df_data.append(df_gbm)\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Divide between X and Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ab = df_data[df_data['Perfomance Matrix']==\"Test Accuracy\"]\n",
    "#ab=ab['macro avg']*100\n",
    "\n",
    "ab['macro avg'] = ab['macro avg'].apply(lambda x: x*100)\n",
    "ab=ab.sort_values(['macro avg']).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax=plt.legend(bbox_to_anchor=(0.1, 1.0, 1., .102),loc=10, ncol=4,  #left, bottom, width, height\n",
    "                title=r'Test Accuracy Comparison Data ')                 \n",
    "ax.get_title().set_fontsize('20')\n",
    "ax = sns.barplot(x='Model1', y='macro avg', data=ab )\n",
    "ax.set(xlabel = 'Model1', ylabel = 'Test_Accuracy')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "     ax.annotate(f\"{p.get_height() }\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "         ha='center', va='center', fontsize=11, color='blue', rotation=0, xytext=(0, 10),\n",
    "         textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = df_data[df_data['Perfomance Matrix']==\"Traing Accuracy\"]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax=plt.legend(bbox_to_anchor=(0.1, 1.0, 1., .102),loc=10, ncol=4,  #left, bottom, width, height\n",
    "                title=r'Traing Accuracy Comparison Data ')                 \n",
    "ax.get_title().set_fontsize('20')\n",
    "ax = sns.barplot(x='Model1', y='macro avg', data=ab )\n",
    "ax.set(xlabel = 'Model1', ylabel = 'Traing Accuracy')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "     ax.annotate(f\"{p.get_height() }\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "         ha='center', va='center', fontsize=11, color='blue', rotation=0, xytext=(0, 10),\n",
    "         textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = df_data[df_data['Perfomance Matrix']==\"Model_Time\"]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax=plt.legend(bbox_to_anchor=(0.1, 1.0, 1., .102),loc=10, ncol=4,  #left, bottom, width, height\n",
    "                title=r'Model_Time Comparison Data ')                 \n",
    "ax.get_title().set_fontsize('20')\n",
    "ax = sns.barplot(x='Model1', y='macro avg', data=ab )\n",
    "ax.set(xlabel = 'Model1', ylabel = 'Model_Time')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "     ax.annotate(f\"{p.get_height() }\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "         ha='center', va='center', fontsize=11, color='blue', rotation=0, xytext=(0, 10),\n",
    "         textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = df_data[df_data['Perfomance Matrix']==\"ROC curve_Avergae\"]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax=plt.legend(bbox_to_anchor=(0.1, 1.0, 1., .102),loc=10, ncol=4,  #left, bottom, width, height\n",
    "                title=r'Model_Time Comparison Data ')                 \n",
    "ax.get_title().set_fontsize('20')\n",
    "ax = sns.barplot(x='Model1', y='macro avg', data=ab )\n",
    "ax.set(xlabel = 'Model1', ylabel = 'ROC curve_Avergae')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "     ax.annotate(f\"{p.get_height() }\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "         ha='center', va='center', fontsize=11, color='blue', rotation=0, xytext=(0, 10),\n",
    "         textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ab=df_data\n",
    "plt.figure(figsize=(12,12))\n",
    "ax=plt.legend(bbox_to_anchor=(0.1, 1.0, 1., .102),loc=10, ncol=4,  #left, bottom, width, height\n",
    "                title=r'Model_Time Comparison Data ')                 \n",
    "ax.get_title().set_fontsize('20')\n",
    "ax = sns.barplot(x='Model1', y='macro avg' , hue = 'Perfomance Matrix' ,data=ab )\n",
    "ax.set(xlabel = 'Model1', ylabel = 'ROC curve_Avergae')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "     ax.annotate(f\"{p.get_height() }\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "         ha='center', va='center', fontsize=11, color='blue', rotation=0, xytext=(0, 10),\n",
    "         textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Devide the data between X and Y\n",
    "X= df_data_Dummy.drop(Output,1)\n",
    "Y=df_data_Dummy[Output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the inputs\n",
    "normalize=Normalizer()\n",
    "#features = \n",
    "X[Features] = normalize.fit_transform(X[Features])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "#X[Features] = scaler.fit_transform(X[Features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print X after normalization\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features1=Features[1]\n",
    "Features2=Features[2]\n",
    "Features3=Features[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X[Features1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X[Features2])\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(X[Features3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data between test and train split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,train_size = 0.7,test_size =0.3,random_state =100)\n",
    "\n",
    "#X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model build & Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *We will be buidling below model and do the comparison between these model* \n",
    "\n",
    "1. Logistics Regression\n",
    "2. KNN Classification Algorithm\n",
    "3. SVM Algorithm\n",
    "4. Random Forest\n",
    "5. GBM\n",
    "5. Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.KNN CLassification Hyper tunning parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the hyperparameter grid \n",
    "\n",
    "import time\n",
    "\n",
    "## hypertune Parametr for KNN Model\n",
    "#n_neighbors=list(range(1, 10))\n",
    "param_grid = dict(n_neighbors=list(range(1,10)))\n",
    "print(param_grid)\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "\n",
    "# instantiate the grid\n",
    "grid = GridSearchCV(knn, param_grid, cv=2, scoring='accuracy')\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "  \n",
    "grid_result =grid.fit(X_train,Y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print(\"Execution time: \" + str((time.time() - start_time)) + ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_result.cv_results_)\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[['param_n_neighbors','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean test and train scoes with alpha \n",
    "\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_n_neighbors'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_n_neighbors'], cv_results['mean_test_score'])\n",
    "plt.xlabel('param_C')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and N Neighbors\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "start_time = time.time()\n",
    "Model1= KNeighborsClassifier(n_neighbors=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model on Train data \n",
    "Model1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1_duration=time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y = label_binarize(Y, classes=[1, 2, 3,4])\n",
    "#n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict train score for training data  \n",
    "Y_train_pred=Model1.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred=Model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix for Logistics regression\n",
    "cm_Model = metrics.confusion_matrix(Y_test,Y_test_pred,labels=[1, 2, 3,4])\n",
    "print(cm_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm_logreg = pd.DataFrame(cm_logreg, index=[\"Sit On Bed\", \"Sit on Chair\", \"Lying\", \"Ambulating\"], columns=[\"Sit On Bed\", \"Sit on Chair\", \"Lying\", \"Ambulating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "class_names=[\"Sit On Bed\", \"Sit on Chair\", \"Lying\", \"Ambulating\"]\n",
    "ax = plot_confusion_matrix(cm_Model,\n",
    "                                colorbar=True,show_normed=True ,class_names=class_names)\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(Model1.score(X_test, Y_test))\n",
    "plt.title(all_sample_title, size = 10);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision**\n",
    "\n",
    " >The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    " \n",
    "**Recall** \n",
    " >The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**F-beta score**\n",
    " >The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classificationReport = classification_report(Y_test, Y_test_pred, target_names=class_names,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classificationReport)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=pd.DataFrame(classificationReport)\n",
    "df_performance=df_performance.reset_index()\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_performance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistics_df\n",
    "df_performance.rename(columns={'index':'Perfomance Matrix'}, inplace=True)\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"Traing Accuracy\" ,'macro avg' :Model1.score(X_train, Y_train) },ignore_index = True)\n",
    "print(df_performance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"Test Accuracy\" ,'macro avg' :Model1.score(X_test, Y_test) },ignore_index = True)\n",
    "print(df_performance)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"Model_Time\" ,'macro avg' :Model1_duration },ignore_index = True)\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check False Positive/Negative True Postive/Negative\n",
    "\n",
    "FP = cm_Model.sum(axis=0) - np.diag(cm_Model)  \n",
    "FN = cm_Model.sum(axis=1) - np.diag(cm_Model)\n",
    "TP = np.diag(cm_Model)\n",
    "TN = cm_Model.sum() - (FP + FN + TP)\n",
    "\n",
    "print(FP,FN,TP,TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "print(TPR)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP)\n",
    "print(TNR)\n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "print(PPV)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "print(NPV)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "print(FPR)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "print(FNR)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "print(FDR)\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)\n",
    "print(ACC)\n",
    "\n",
    "PLR = TPR/(1-TNR)\n",
    "print(PLR)\n",
    "\n",
    "\n",
    "NLR =TNR/(1-TPR)\n",
    "print(NLR)\n",
    "\n",
    "OMR=(FP+FN)/(TP+FN+FP+TN)\n",
    "print(OMR)\n",
    "\n",
    "NNE=1/PPV\n",
    "\n",
    "print(NNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"Specificity\" ,'Sit On Bed' :TNR[0] ,'Sit on Chair':TNR[1] ,'Lying':TNR[2],'Ambulating':TNR[3]},ignore_index = True)\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"Negative predictive value\" ,'Sit On Bed' :NPV[0] ,'Sit on Chair':NPV[1] ,'Lying':NPV[2],'Ambulating':NPV[3]},ignore_index = True)\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"Positive Likelihood Ratio\" ,'Sit On Bed' :PLR[0] ,'Sit on Chair':PLR[1] ,'Lying':PLR[2],'Ambulating':PLR[3]},ignore_index = True)\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"Negative likelihood ratio \" ,'Sit On Bed' :NLR[0] ,'Sit on Chair':NLR[1] ,'Lying':NLR[2],'Ambulating':NLR[3]},ignore_index = True)\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"Overall Misclassification Ratio\" ,'Sit On Bed' :OMR[0] ,'Sit on Chair':OMR[1] ,'Lying':OMR[2],'Ambulating':OMR[3]},ignore_index = True)\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "y = label_binarize(Y, classes=[1, 2, 3,4])\n",
    "y_test=label_binarize(Y_test, classes=[1, 2, 3,4])\n",
    "n_classes = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "y_probas = Model1.predict_proba(X_test)\n",
    "skplt.metrics.plot_roc(Y_test, y_probas, figsize=(10, 8))   # Plot ROC Curve\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_test, y_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance=df_performance.append({'Perfomance Matrix':\"ROC curve_Avergae\" ,'macro avg':auc},ignore_index = True)\n",
    "print(df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                        y_probas[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], y_probas[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
    "   y_probas.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_test, y_probas,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance['Model1']=\"KNN Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performance.to_csv(\"KNN_Performance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 4.2. K-Nearest Neighbors(KNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hypertune Parametr for KNN Model\n",
    "#n_neighbors=list(range(1, 10))\n",
    "param_grid = dict(n_neighbors=list(range(1,10)))\n",
    "print(param_grid)\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "\n",
    "# instantiate the grid\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n",
    "\n",
    "# fit the grid with data\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean test and train scoes with alpha \n",
    "\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_n_neighbors'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_n_neighbors'], cv_results['mean_test_score'])\n",
    "plt.xlabel('param_n_neighbors')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Best model Parameter is 1****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create KNN model \n",
    "knn=KNeighborsClassifier(n_neighbors=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit model on Train data \n",
    "knn.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict output value for test data\n",
    "test_pred2 = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict output value for Train data\n",
    "train_pred2=knn.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the Accuracy on test date set and train data set\n",
    "print('Accuracy of KNN regression classifier on test set: {:.2f}'.format(knn.score(X_train, Y_train)))\n",
    "print('Accuracy of KNN regression classifier on test set: {:.2f}'.format(knn.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy=Accuracy.append({'Model Name':\"KNN Regression\", 'Training Accuracy':knn.score(X_train, Y_train),'Test_Accuracy':knn.score(X_test,Y_test)},ignore_index=True)\n",
    "print(Accuracy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix for Logistics regression\n",
    "cm_knn = metrics.confusion_matrix(Y_test,test_pred2)\n",
    "print(cm_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the Confusion Matrix :-\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm_knn, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for KNN model: {0}'.format(knn.score(X_test, Y_test))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision**\n",
    "\n",
    " >The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    " \n",
    "**Recall** \n",
    " >The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**F-beta score**\n",
    " >The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print Classfication matrix \n",
    "print(classification_report(Y_test, test_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> #### 4.3.Random Forest Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hypertune Parametr for KNN Model\n",
    "#n_neighbors=list(range(1, 10))\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [1,5,10,15,20,25,30],\n",
    "    }\n",
    "\n",
    "print(param_grid)\n",
    "\n",
    "Rnn=RandomForestClassifier()\n",
    "\n",
    "# instantiate the grid\n",
    "grid = GridSearchCV(Rnn, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# fit the grid with data\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting mean test and train scoes with alpha \n",
    "\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_max_depth'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_max_depth'], cv_results['mean_test_score'])\n",
    "plt.xlabel('param_max_depth')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and Depth\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest Model\n",
    "rfc = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fit model on Train data \n",
    "rfc.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict output value for test data\n",
    "test_pred3 = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict output value for Train data\n",
    "train_pred3=rfc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the Accuracy on test date set and train data set\n",
    "print('Accuracy of Random Forest classifier on train set: {:.2f}'.format(rfc.score(X_train, Y_train)))\n",
    "print('Accuracy of Random Forest  classifier on test set: {:.2f}'.format(rfc.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy=Accuracy.append({'Model Name':\"Random Forest\", 'Training Accuracy':rfc.score(X_train, Y_train),'Test_Accuracy':rfc.score(X_test,Y_test)},ignore_index=True)\n",
    "print(Accuracy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix for Logistics regression\n",
    "cm_rfc = metrics.confusion_matrix(Y_test,test_pred3)\n",
    "print(cm_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the Confusion Matrix :-\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm_rfc, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for Random Forest: {0}'.format(rfc.score(X_test, Y_test))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision**\n",
    "\n",
    " >The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    " \n",
    "**Recall** \n",
    " >The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**F-beta score**\n",
    " >The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print Classfication matrix \n",
    "print(classification_report(Y_test, test_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> #### 4.4 SVM (Support Vector Machine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hypertune Parametr for KNN Model\n",
    "#n_neighbors=list(range(1, 10))\n",
    "# specify range of parameters (C) as a list\n",
    "params = {\"C\": [0.1, 1, 10]}\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "# set up grid search scheme\n",
    "# note that we are still using the 5 fold CV scheme we set up earlier\n",
    "grid = GridSearchCV(estimator = svc, param_grid = params, \n",
    "                        scoring= 'accuracy', \n",
    "                        cv = 5, \n",
    "                        verbose = 1,\n",
    "                       return_train_score=True)   \n",
    "# fit the grid with data\n",
    "grid.fit(X_train, Y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of C versus train and test scores\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
    "plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear SVM Model \n",
    "\n",
    "svm = SVC(kernel='rbf',C=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fit model on Train data \n",
    "svm.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict output value for test data\n",
    "test_pred4 = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict output value for Train data\n",
    "train_pred4=svm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the Accuracy on test date set and train data set\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'.format(svm.score(X_train, Y_train)))\n",
    "print('Accuracy of SVM  classifier on test set: {:.2f}'.format(svm.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy=Accuracy.append({'Model Name':\"SVM\", 'Training Accuracy':svm.score(X_train, Y_train),'Test_Accuracy':svm.score(X_test,Y_test)},ignore_index=True)\n",
    "print(Accuracy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix for Logistics regression\n",
    "cm_svm = metrics.confusion_matrix(Y_test,test_pred4)\n",
    "print(cm_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the Confusion Matrix :-\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm_svm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for SVMt: {0}'.format(rfc.score(X_test, Y_test))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision**\n",
    "\n",
    " >The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    " \n",
    "**Recall** \n",
    " >The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**F-beta score**\n",
    " >The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print Classfication matrix \n",
    "print(classification_report(Y_test, test_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 5.Gradient Boosting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "for learning_rate in lr_list:\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n",
    "    gb_clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, Y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Model\n",
    "\n",
    "gbm = GradientBoostingClassifier(n_estimators=20, learning_rate=0.5, max_features=2, max_depth=2, random_state=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fit model on Train data \n",
    "gbm.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict output value for test data\n",
    "test_pred5 = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict output value for Train data\n",
    "train_pred5=gbm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the Accuracy on test date set and train data set\n",
    "print('Accuracy of GBM classifier on test set: {:.2f}'.format(gbm.score(X_train, Y_train)))\n",
    "print('Accuracy of GBM  classifier on test set: {:.2f}'.format(gbm.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy=Accuracy.append({'Model Name':\"GBM\", 'Training Accuracy':gbm.score(X_train, Y_train),'Test_Accuracy':gbm.score(X_test,Y_test)},ignore_index=True)\n",
    "print(Accuracy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix for Logistics regression\n",
    "cm_gbm = metrics.confusion_matrix(Y_test,test_pred5)\n",
    "print(cm_gbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the Confusion Matrix :-\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm_gbm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for GBM: {0}'.format(gbm.score(X_test, Y_test))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision**\n",
    "\n",
    " >The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    " \n",
    "**Recall** \n",
    " >The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**F-beta score**\n",
    " >The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print Classfication matrix \n",
    "print(classification_report(Y_test, test_pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 6. Deep Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build DNN model by using Keras\n",
    "\n",
    "dnn_model = Sequential()\n",
    "## Add input layer\n",
    "\n",
    "dnn_model.add(Dense(500, activation='relu', input_dim=8))\n",
    "## Add hidden layer\n",
    "dnn_model.add(Dense(100, activation='relu'))\n",
    "##Add 2nd hidden layer\n",
    "dnn_model.add(Dense(50, activation='relu'))\n",
    "### Add output layer\n",
    "dnn_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "dnn_model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fil model on Training data \n",
    "dnn_model.fit(X_train, Y_train, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fit model on Train data \n",
    "dnn_model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict test output\n",
    "pred_test6= dnn_model.predict(X_test)\n",
    "print(pred_test6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Check the Accuracy on train data set\n",
    "scores = dnn_model.evaluate(X_train, Y_train, verbose=0)\n",
    "print('Accuracy on training data: {}% \\n Error on training data: {}'.format(scores[1], 1 - scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Check the Accuracy on train data set\n",
    "scores2 = dnn_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Accuracy on test data: {}% \\n Error on test data: {}'.format(scores2[1], 1 - scores2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy = Accuracy.append({'Model Name':\"DNN \", 'Training Accuracy':(scores2[1]),'Test_Accuracy':(scores[1])},ignore_index=True)\n",
    "print(Accuracy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pred_test61= [0 for x in range(22539)] \n",
    "for i in range(0,22539):\n",
    "    pred_test61[i]= (np.argmax(pred_test6[i]))\n",
    "    \n",
    "print(pred_test61)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion matrix for DNN Model\n",
    "cm_dnn = metrics.confusion_matrix(Y_test,pred_test61)\n",
    "print(cm_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the Confusion Matrix :-\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm_dnn, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score for DNN: {}'.format(scores2[1], 1 - scores2[1])\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**precision**\n",
    "\n",
    " >The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    " \n",
    "**Recall** \n",
    " >The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "**F-beta score**\n",
    " >The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print Classfication matrix \n",
    "print(classification_report(Y_test, pred_test61))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy.plot(kind= 'bar', x= 'Model Name', y='Training Accuracy' )\n",
    "\n",
    "\n",
    "ab = Accuracy\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax=plt.legend(bbox_to_anchor=(0.1, 1.0, 1., .102),loc=10, ncol=4,  #left, bottom, width, height\n",
    "                title=r'Training Accuracy Comparison Data ')                 \n",
    "ax.get_title().set_fontsize('20')\n",
    "ax = sns.barplot(x='Model Name', y='Training Accuracy', data=ab )\n",
    "ax.set(xlabel = 'Model Name', ylabel = 'Training Accuracy')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "     ax.annotate(f\"{p.get_height() }\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "         ha='center', va='center', fontsize=11, color='blue', rotation=0, xytext=(0, 10),\n",
    "         textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy.plot(kind= 'bar', x= 'Model Name', y='Training Accuracy' )\n",
    "\n",
    "\n",
    "ab = Accuracy\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax=plt.legend(bbox_to_anchor=(0.1, 1.0, 1., .102),loc=10, ncol=4,  #left, bottom, width, height\n",
    "                title=r'Test Accuracy Comparison Data ')                 \n",
    "ax.get_title().set_fontsize('20')\n",
    "ax = sns.barplot(x='Model Name', y='Test_Accuracy', data=ab )\n",
    "ax.set(xlabel = 'Model Name', ylabel = 'Test_Accuracy')\n",
    "ax.grid(True)\n",
    "for p in ax.patches:\n",
    "     ax.annotate(f\"{p.get_height() }\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "         ha='center', va='center', fontsize=11, color='blue', rotation=0, xytext=(0, 10),\n",
    "         textcoords='offset points') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
